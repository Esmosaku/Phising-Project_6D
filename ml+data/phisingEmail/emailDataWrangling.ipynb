{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b52849d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tldextract\n",
      "  Using cached tldextract-5.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting idna (from tldextract)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting requests>=2.1.0 (from tldextract)\n",
      "  Using cached requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting requests-file>=1.4 (from tldextract)\n",
      "  Using cached requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting filelock>=3.0.8 (from tldextract)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests>=2.1.0->tldextract)\n",
      "  Using cached charset_normalizer-3.4.2-cp311-cp311-macosx_10_9_universal2.whl.metadata (35 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.1.0->tldextract)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.1.0->tldextract)\n",
      "  Using cached certifi-2025.7.14-py3-none-any.whl.metadata (2.4 kB)\n",
      "Using cached tldextract-5.3.0-py3-none-any.whl (107 kB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Using cached requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
      "Using cached certifi-2025.7.14-py3-none-any.whl (162 kB)\n",
      "Using cached charset_normalizer-3.4.2-cp311-cp311-macosx_10_9_universal2.whl (198 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Installing collected packages: urllib3, idna, filelock, charset_normalizer, certifi, requests, requests-file, tldextract\n",
      "Successfully installed certifi-2025.7.14 charset_normalizer-3.4.2 filelock-3.18.0 idna-3.10 requests-2.32.4 requests-file-2.1.0 tldextract-5.3.0 urllib3-2.5.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting xgboost\n",
      "  Using cached xgboost-3.0.2-py3-none-macosx_12_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting numpy (from xgboost)\n",
      "  Downloading numpy-2.3.2-cp311-cp311-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting scipy (from xgboost)\n",
      "  Downloading scipy-1.16.1-cp311-cp311-macosx_14_0_arm64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached xgboost-3.0.2-py3-none-macosx_12_0_arm64.whl (2.0 MB)\n",
      "Downloading numpy-2.3.2-cp311-cp311-macosx_14_0_arm64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.16.1-cp311-cp311-macosx_14_0_arm64.whl (20.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy, scipy, xgboost\n",
      "Successfully installed numpy-2.3.2 scipy-1.16.1 xgboost-3.0.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.3.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (91 kB)\n",
      "Requirement already satisfied: numpy in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (2.3.2)\n",
      "Collecting beautifulsoup4\n",
      "  Using cached beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting textblob\n",
      "  Using cached textblob-0.19.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.7.1-cp311-cp311-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4)\n",
      "  Using cached soupsieve-2.7-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from beautifulsoup4) (4.14.1)\n",
      "Collecting nltk>=3.9 (from textblob)\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from scikit-learn) (1.16.1)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting click (from nltk>=3.9->textblob)\n",
      "  Using cached click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk>=3.9->textblob)\n",
      "  Using cached regex-2024.11.6-cp311-cp311-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting tqdm (from nltk>=3.9->textblob)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Using cached pandas-2.3.1-cp311-cp311-macosx_11_0_arm64.whl (10.8 MB)\n",
      "Using cached beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
      "Using cached textblob-0.19.0-py3-none-any.whl (624 kB)\n",
      "Using cached scikit_learn-1.7.1-cp311-cp311-macosx_12_0_arm64.whl (8.7 MB)\n",
      "Using cached joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached soupsieve-2.7-py3-none-any.whl (36 kB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached regex-2024.11.6-cp311-cp311-macosx_11_0_arm64.whl (284 kB)\n",
      "Using cached click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: pytz, tzdata, tqdm, threadpoolctl, soupsieve, regex, joblib, click, scikit-learn, pandas, nltk, beautifulsoup4, textblob\n",
      "Successfully installed beautifulsoup4-4.13.4 click-8.2.1 joblib-1.5.1 nltk-3.9.1 pandas-2.3.1 pytz-2025.2 regex-2024.11.6 scikit-learn-1.7.1 soupsieve-2.7 textblob-0.19.0 threadpoolctl-3.6.0 tqdm-4.67.1 tzdata-2025.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.10.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Using cached fonttools-4.59.0-cp311-cp311-macosx_10_9_universal2.whl.metadata (107 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.8-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from matplotlib) (2.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from matplotlib) (25.0)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Using cached pillow-11.3.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.0 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Using cached pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Using cached matplotlib-3.10.3-cp311-cp311-macosx_11_0_arm64.whl (8.1 MB)\n",
      "Downloading contourpy-1.3.3-cp311-cp311-macosx_11_0_arm64.whl (270 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.1/270.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hUsing cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.59.0-cp311-cp311-macosx_10_9_universal2.whl (2.8 MB)\n",
      "Using cached kiwisolver-1.4.8-cp311-cp311-macosx_11_0_arm64.whl (65 kB)\n",
      "Using cached pillow-11.3.0-cp311-cp311-macosx_11_0_arm64.whl (4.7 MB)\n",
      "Using cached pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.3 cycler-0.12.1 fonttools-4.59.0 kiwisolver-1.4.8 matplotlib-3.10.3 pillow-11.3.0 pyparsing-3.2.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting seaborn\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from seaborn) (2.3.2)\n",
      "Requirement already satisfied: pandas>=1.2 in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from seaborn) (2.3.1)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from seaborn) (3.10.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: scipy in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (1.16.1)\n",
      "Requirement already satisfied: numpy<2.6,>=1.25.2 in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from scipy) (2.3.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: nltk in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from nltk) (4.67.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting wordcloud\n",
      "  Using cached wordcloud-1.9.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from wordcloud) (2.3.2)\n",
      "Requirement already satisfied: pillow in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from wordcloud) (11.3.0)\n",
      "Requirement already satisfied: matplotlib in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from wordcloud) (3.10.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from matplotlib->wordcloud) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from matplotlib->wordcloud) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from matplotlib->wordcloud) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from matplotlib->wordcloud) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from matplotlib->wordcloud) (25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from matplotlib->wordcloud) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from matplotlib->wordcloud) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.17.0)\n",
      "Using cached wordcloud-1.9.4-cp311-cp311-macosx_11_0_arm64.whl (167 kB)\n",
      "Installing collected packages: wordcloud\n",
      "Successfully installed wordcloud-1.9.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: scipy in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (1.16.1)\n",
      "Requirement already satisfied: numpy<2.6,>=1.25.2 in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from scipy) (2.3.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: nltk in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from nltk) (4.67.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: wordcloud in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (1.9.4)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from wordcloud) (2.3.2)\n",
      "Requirement already satisfied: pillow in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from wordcloud) (11.3.0)\n",
      "Requirement already satisfied: matplotlib in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from wordcloud) (3.10.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from matplotlib->wordcloud) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from matplotlib->wordcloud) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from matplotlib->wordcloud) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from matplotlib->wordcloud) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from matplotlib->wordcloud) (25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from matplotlib->wordcloud) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from matplotlib->wordcloud) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.54.0-py3-none-any.whl.metadata (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m537.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:--:--\u001b[0m\n",
      "\u001b[?25hCollecting torch\n",
      "  Using cached torch-2.7.1-cp311-none-macosx_11_0_arm64.whl.metadata (29 kB)\n",
      "Requirement already satisfied: filelock in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from transformers) (3.18.0)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.34.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from transformers) (2.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from transformers) (25.0)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Using cached PyYAML-6.0.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from transformers) (2.32.4)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.2-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from torch) (4.14.1)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Using cached fsspec-2025.7.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Using cached hf_xet-1.1.5-cp37-abi3-macosx_11_0_arm64.whl.metadata (879 bytes)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Using cached MarkupSafe-3.0.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from requests->transformers) (2025.7.14)\n",
      "Downloading transformers-4.54.0-py3-none-any.whl (11.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached torch-2.7.1-cp311-none-macosx_11_0_arm64.whl (68.6 MB)\n",
      "Downloading huggingface_hub-0.34.1-py3-none-any.whl (558 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m558.8/558.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached fsspec-2025.7.0-py3-none-any.whl (199 kB)\n",
      "Using cached PyYAML-6.0.2-cp311-cp311-macosx_11_0_arm64.whl (172 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl (418 kB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached tokenizers-0.21.2-cp39-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "Using cached hf_xet-1.1.5-cp37-abi3-macosx_11_0_arm64.whl (2.6 MB)\n",
      "Using cached MarkupSafe-3.0.2-cp311-cp311-macosx_11_0_arm64.whl (12 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, safetensors, pyyaml, networkx, MarkupSafe, hf-xet, fsspec, jinja2, huggingface-hub, torch, tokenizers, transformers\n",
      "Successfully installed MarkupSafe-3.0.2 fsspec-2025.7.0 hf-xet-1.1.5 huggingface-hub-0.34.1 jinja2-3.1.6 mpmath-1.3.0 networkx-3.5 pyyaml-6.0.2 safetensors-0.5.3 sympy-1.14.0 tokenizers-0.21.2 torch-2.7.1 transformers-4.54.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tldextract\n",
    "!pip install xgboost\n",
    "!pip install pandas numpy beautifulsoup4 textblob scikit-learn\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install scipy\n",
    "!pip install nltk\n",
    "!pip install wordcloud\n",
    "!pip install scipy\n",
    "!pip install nltk\n",
    "!pip install wordcloud\n",
    "!pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7171b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import urllib.parse\n",
    "from collections import Counter\n",
    "import tldextract\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b4a1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating phising email dataset\n",
    "#Utilizing `7 Email Phising Datasets` and merging them into a single dataset (https://figshare.com/articles/dataset/Seven_Phishing_Email_Datasets/25432108)\n",
    "import glob\n",
    "\n",
    "# Path to your datasets; adjust the pattern if they are in a different folder or format\n",
    "dataset_files = glob.glob(\"7PhisingEmailsDataset/*.csv\")  # Example: all CSVs in a 'datasets' folder\n",
    "\n",
    "# List for storing the reduced DataFrames\n",
    "dataframes = []\n",
    "\n",
    "for file in dataset_files:\n",
    "    try:\n",
    "        # Read only the columns you need; ignore others (extra columns will be dropped)\n",
    "        df = pd.read_csv(file, usecols=['subject', 'body', 'label'])\n",
    "        dataframes.append(df)\n",
    "        print(f\"Loaded {file} with {len(df)} rows\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file}: {e}\")\n",
    "\n",
    "# Concatenate all reduced DataFrames into one\n",
    "merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "print(f\"\\nMerged dataset shape: {merged_df.shape}\")\n",
    "\n",
    "# Save as CSV\n",
    "merged_df.to_csv(\"merged_phishing_emails.csv\", index=False)\n",
    "print(\"Saved merged dataset to merged_phishing_emails.csv\")\n",
    "\n",
    "#Preview first few rows\n",
    "print(\"\\nSample data:\")\n",
    "print(merged_df.head())\n",
    "print(merged_df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff135810",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.8' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: '/Users/agamjotsingh/.pyenv/versions/3.11.8/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "#Next step is to take a look a the dataset and see if:\n",
    "    #1. The dataset is balanced\n",
    "    #2. The dataset is clean\n",
    "\n",
    "#Load the dataset\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv(\"merged_phishing_emails.csv\")\n",
    "print(f\"Dataframe Shape Before Making Changes: {df.shape}\")\n",
    "#Preview the dataset\n",
    "df.isnull().sum()\n",
    "df.dropna(inplace=True)\n",
    "df.info()\n",
    "df.head()\n",
    "\n",
    "#Check for duplicates\n",
    "df.duplicated().sum()\n",
    "df.drop_duplicates(inplace=True)\n",
    "print(f\"Dataframe Shape After Making Changes: {df.shape}\")\n",
    "\n",
    "#Check for balance \n",
    "df['label'].value_counts() \n",
    "\n",
    "df.to_csv(\"cleaned_phishing_emails.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "636d5c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting engineered features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c3/9wn9lyx93sj26z0b2wprxj000000gn/T/ipykernel_64356/1296620527.py:51: MarkupResemblesLocatorWarning: The input passed in on this line looks more like a URL than HTML or XML.\n",
      "\n",
      "If you meant to use Beautiful Soup to parse the web page found at a certain URL, then something has gone wrong. You should use an Python package like 'requests' to fetch the content behind the URL. Once you have the content as a string, you can feed that string into Beautiful Soup.\n",
      "\n",
      "However, if you want to parse some data that happens to look like a URL, then nothing has gone wrong: you are using Beautiful Soup correctly, and this warning is spurious and can be filtered. To make this warning go away, run this code before calling the BeautifulSoup constructor:\n",
      "\n",
      "    from bs4 import MarkupResemblesLocatorWarning\n",
      "    import warnings\n",
      "\n",
      "    warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
      "    \n",
      "  soup = BeautifulSoup(text, 'html.parser')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining subject and body for BERT embeddings...\n",
      "Loading BERT model and tokenizer...\n",
      "Generating BERT embeddings...\n",
      "Processing text 1/200609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/agamjotsingh/.pyenv/versions/3.11.8/lib/python3.11/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text 1001/200609\n",
      "Processing text 2001/200609\n",
      "Processing text 3001/200609\n",
      "Processing text 4001/200609\n",
      "Processing text 5001/200609\n",
      "Processing text 6001/200609\n",
      "Processing text 7001/200609\n",
      "Processing text 8001/200609\n",
      "Processing text 9001/200609\n",
      "Processing text 10001/200609\n",
      "Processing text 11001/200609\n",
      "Processing text 12001/200609\n",
      "Processing text 13001/200609\n",
      "Processing text 14001/200609\n",
      "Processing text 15001/200609\n",
      "Processing text 16001/200609\n",
      "Processing text 17001/200609\n",
      "Processing text 18001/200609\n",
      "Processing text 19001/200609\n",
      "Processing text 20001/200609\n",
      "Processing text 21001/200609\n",
      "Processing text 22001/200609\n",
      "Processing text 23001/200609\n",
      "Processing text 24001/200609\n",
      "Processing text 25001/200609\n",
      "Processing text 26001/200609\n",
      "Processing text 27001/200609\n",
      "Processing text 28001/200609\n",
      "Processing text 29001/200609\n",
      "Processing text 30001/200609\n",
      "Processing text 31001/200609\n",
      "Processing text 32001/200609\n",
      "Processing text 33001/200609\n",
      "Processing text 34001/200609\n",
      "Processing text 35001/200609\n",
      "Processing text 36001/200609\n",
      "Processing text 37001/200609\n",
      "Processing text 38001/200609\n",
      "Processing text 39001/200609\n",
      "Processing text 40001/200609\n",
      "Processing text 41001/200609\n",
      "Processing text 42001/200609\n",
      "Processing text 43001/200609\n",
      "Processing text 44001/200609\n",
      "Processing text 45001/200609\n",
      "Processing text 46001/200609\n",
      "Processing text 47001/200609\n",
      "Processing text 48001/200609\n",
      "Processing text 49001/200609\n",
      "Processing text 50001/200609\n",
      "Processing text 51001/200609\n",
      "Processing text 52001/200609\n",
      "Processing text 53001/200609\n",
      "Processing text 54001/200609\n",
      "Processing text 55001/200609\n",
      "Processing text 56001/200609\n",
      "Processing text 57001/200609\n",
      "Processing text 58001/200609\n",
      "Processing text 59001/200609\n",
      "Processing text 60001/200609\n",
      "Processing text 61001/200609\n",
      "Processing text 62001/200609\n",
      "Processing text 63001/200609\n",
      "Processing text 64001/200609\n",
      "Processing text 65001/200609\n",
      "Processing text 66001/200609\n",
      "Processing text 67001/200609\n",
      "Processing text 68001/200609\n",
      "Processing text 69001/200609\n",
      "Processing text 70001/200609\n",
      "Processing text 71001/200609\n",
      "Processing text 72001/200609\n",
      "Processing text 73001/200609\n",
      "Processing text 74001/200609\n",
      "Processing text 75001/200609\n",
      "Processing text 76001/200609\n",
      "Processing text 77001/200609\n",
      "Processing text 78001/200609\n",
      "Processing text 79001/200609\n",
      "Processing text 80001/200609\n",
      "Processing text 81001/200609\n",
      "Processing text 82001/200609\n",
      "Processing text 83001/200609\n",
      "Processing text 84001/200609\n",
      "Processing text 85001/200609\n",
      "Processing text 86001/200609\n",
      "Processing text 87001/200609\n",
      "Processing text 88001/200609\n",
      "Processing text 89001/200609\n",
      "Processing text 90001/200609\n",
      "Processing text 91001/200609\n",
      "Processing text 92001/200609\n",
      "Processing text 93001/200609\n",
      "Processing text 94001/200609\n",
      "Processing text 95001/200609\n",
      "Processing text 96001/200609\n",
      "Processing text 97001/200609\n",
      "Processing text 98001/200609\n",
      "Processing text 99001/200609\n",
      "Processing text 100001/200609\n",
      "Processing text 101001/200609\n",
      "Processing text 102001/200609\n",
      "Processing text 103001/200609\n",
      "Processing text 104001/200609\n",
      "Processing text 105001/200609\n",
      "Processing text 106001/200609\n",
      "Processing text 107001/200609\n",
      "Processing text 108001/200609\n",
      "Processing text 109001/200609\n",
      "Processing text 110001/200609\n",
      "Processing text 111001/200609\n",
      "Processing text 112001/200609\n",
      "Processing text 113001/200609\n",
      "Processing text 114001/200609\n",
      "Processing text 115001/200609\n",
      "Processing text 116001/200609\n",
      "Processing text 117001/200609\n",
      "Processing text 118001/200609\n",
      "Processing text 119001/200609\n",
      "Processing text 120001/200609\n",
      "Processing text 121001/200609\n",
      "Processing text 122001/200609\n",
      "Processing text 123001/200609\n",
      "Processing text 124001/200609\n",
      "Processing text 125001/200609\n",
      "Processing text 126001/200609\n",
      "Processing text 127001/200609\n",
      "Processing text 128001/200609\n",
      "Processing text 129001/200609\n",
      "Processing text 130001/200609\n",
      "Processing text 131001/200609\n",
      "Processing text 132001/200609\n",
      "Processing text 133001/200609\n",
      "Processing text 134001/200609\n",
      "Processing text 135001/200609\n",
      "Processing text 136001/200609\n",
      "Processing text 137001/200609\n",
      "Processing text 138001/200609\n",
      "Processing text 139001/200609\n",
      "Processing text 140001/200609\n",
      "Processing text 141001/200609\n",
      "Processing text 142001/200609\n",
      "Processing text 143001/200609\n",
      "Processing text 144001/200609\n",
      "Processing text 145001/200609\n",
      "Processing text 146001/200609\n",
      "Processing text 147001/200609\n",
      "Processing text 148001/200609\n",
      "Processing text 149001/200609\n",
      "Processing text 150001/200609\n",
      "Processing text 151001/200609\n",
      "Processing text 152001/200609\n",
      "Processing text 153001/200609\n",
      "Processing text 154001/200609\n",
      "Processing text 155001/200609\n",
      "Processing text 156001/200609\n",
      "Processing text 157001/200609\n",
      "Processing text 158001/200609\n",
      "Processing text 159001/200609\n",
      "Processing text 160001/200609\n",
      "Processing text 161001/200609\n",
      "Processing text 162001/200609\n",
      "Processing text 163001/200609\n",
      "Processing text 164001/200609\n",
      "Processing text 165001/200609\n",
      "Processing text 166001/200609\n",
      "Processing text 167001/200609\n",
      "Processing text 168001/200609\n",
      "Processing text 169001/200609\n",
      "Processing text 170001/200609\n",
      "Processing text 171001/200609\n",
      "Processing text 172001/200609\n",
      "Processing text 173001/200609\n",
      "Processing text 174001/200609\n",
      "Processing text 175001/200609\n",
      "Processing text 176001/200609\n",
      "Processing text 177001/200609\n",
      "Processing text 178001/200609\n",
      "Processing text 179001/200609\n",
      "Processing text 180001/200609\n",
      "Processing text 181001/200609\n",
      "Processing text 182001/200609\n",
      "Processing text 183001/200609\n",
      "Processing text 184001/200609\n",
      "Processing text 185001/200609\n",
      "Processing text 186001/200609\n",
      "Processing text 187001/200609\n",
      "Processing text 188001/200609\n",
      "Processing text 189001/200609\n",
      "Processing text 190001/200609\n",
      "Processing text 191001/200609\n",
      "Processing text 192001/200609\n",
      "Processing text 193001/200609\n",
      "Processing text 194001/200609\n",
      "Processing text 195001/200609\n",
      "Processing text 196001/200609\n",
      "Processing text 197001/200609\n",
      "Processing text 198001/200609\n",
      "Processing text 199001/200609\n",
      "Processing text 200001/200609\n",
      "Dataset saved as final_phishing_dataset.csv\n",
      "Dataset shape: (200609, 790)\n",
      "Engineered features: 21\n",
      "BERT features: 768\n",
      "Total features: 789\n",
      "BERT model info saved as bert_info.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from textblob import TextBlob\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "def count_keywords(text):\n",
    "    # counts occurrences of suspicious keywords in text\n",
    "    keywords = [\"urgent\", \"verify your account\", \"click here\", \"login now\", \"password reset\",\n",
    "                \"account suspended\", \"update your information\", \"confirm your identity\",\n",
    "                \"secure your account\", \"action required\"]\n",
    "    text = text.lower()\n",
    "    counts = {}\n",
    "    for word in keywords:\n",
    "        counts[\"count_\" + word.replace(\" \", \"_\")] = len(re.findall(r'\\b' + word + r'\\b', text))\n",
    "    return counts\n",
    "\n",
    "def check_greeting(text):\n",
    "    # checks for generic greetings in first 200 characters\n",
    "    greetings = [\"dear customer\", \"dear user\", \"hello sir\", \"hello madam\", \"dear client\"]\n",
    "    first_bit = text.lower()[:200]\n",
    "    for greeting in greetings:\n",
    "        if greeting in first_bit:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def get_sentiment(text):\n",
    "    # computes sentiment polarity and subjectivity\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity, blob.sentiment.subjectivity\n",
    "\n",
    "def persuasion_cues(text):\n",
    "    # counts gain and loss persuasion phrases\n",
    "    good_phrases = [\"win\", \"prize\", \"bonus\", \"reward\"]\n",
    "    bad_phrases = [\"lose\", \"suspended\", \"locked\", \"expired\"]\n",
    "    text = text.lower()\n",
    "    good_count = 0\n",
    "    bad_count = 0\n",
    "    for phrase in good_phrases:\n",
    "        good_count += len(re.findall(r'\\b' + phrase + r'\\b', text))\n",
    "    for phrase in bad_phrases:\n",
    "        bad_count += len(re.findall(r'\\b' + phrase + r'\\b', text))\n",
    "    return good_count, bad_count\n",
    "\n",
    "def get_lengths(subject, body):\n",
    "    return len(subject), len(body.split())\n",
    "\n",
    "def count_html_tags(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    return len(soup.find_all())\n",
    "\n",
    "def count_urls(text):\n",
    "    url_pattern = r'https?://[^\\s<>\"]+|www\\.[^\\s<>\"]+'\n",
    "    return len(re.findall(url_pattern, text))\n",
    "\n",
    "def count_attachments(text):\n",
    "    return text.lower().count(\"content-disposition: attachment\")\n",
    "\n",
    "def count_exclamation(text):\n",
    "    return text.count(\"!\")\n",
    "\n",
    "def get_bert_embeddings(texts, model_name=\"bert-base-uncased\", max_length=512):\n",
    "    print(\"Loading BERT model and tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    embeddings = []\n",
    "    \n",
    "    print(\"Generating BERT embeddings...\")\n",
    "    for i, text in enumerate(texts):\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"Processing text {i+1}/{len(texts)}\")\n",
    "        \n",
    "        # Tokenize and encode the text\n",
    "        inputs = tokenizer(\n",
    "            text, \n",
    "            return_tensors=\"pt\", \n",
    "            truncation=True, \n",
    "            max_length=max_length, \n",
    "            padding=True\n",
    "        )\n",
    "        \n",
    "        # Generate embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            # Use the [CLS] token embedding (first token) as the sentence representation\n",
    "            embedding = outputs.last_hidden_state[:, 0, :].numpy().flatten()\n",
    "            embeddings.append(embedding)\n",
    "    \n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('cleaned_phishing_emails.csv')\n",
    "all_features = []\n",
    "\n",
    "print(\"Extracting engineered features...\")\n",
    "for index, row in data.iterrows():\n",
    "    subject = row['subject']\n",
    "    body = row['body']\n",
    "    features = {}\n",
    "\n",
    "    features.update(count_keywords(subject + ' ' + body))\n",
    "    features['generic_greeting'] = check_greeting(body)\n",
    "    polarity, subjectivity = get_sentiment(body)\n",
    "    features['polarity'] = polarity\n",
    "    features['subjectivity'] = subjectivity\n",
    "    good_count, bad_count = persuasion_cues(body)\n",
    "    features['good_phrases'] = good_count\n",
    "    features['bad_phrases'] = bad_count\n",
    "    sub_len, body_len = get_lengths(subject, body)\n",
    "    features['subject_length'] = sub_len\n",
    "    features['body_length'] = body_len\n",
    "    features['html_tags'] = count_html_tags(body)\n",
    "    features['url_count'] = count_urls(body)\n",
    "    features['attachment_count'] = count_attachments(body)\n",
    "    features['exclamation_count'] = count_exclamation(subject + ' ' + body)\n",
    "\n",
    "    all_features.append(features)\n",
    "\n",
    "features_df = pd.DataFrame(all_features)\n",
    "\n",
    "# Generate BERT embeddings for the combined subject and body text\n",
    "print(\"Combining subject and body for BERT embeddings...\")\n",
    "combined_texts = data['subject'] + ' ' + data['body']\n",
    "\n",
    "# Generate BERT embeddings\n",
    "bert_embeddings = get_bert_embeddings(combined_texts)\n",
    "\n",
    "# Create DataFrame for BERT embeddings\n",
    "bert_columns = [f'bert_dim_{i}' for i in range(bert_embeddings.shape[1])]\n",
    "bert_df = pd.DataFrame(bert_embeddings, columns=bert_columns)\n",
    "\n",
    "# Combine engineered features with BERT embeddings\n",
    "final_df = pd.concat([features_df, bert_df], axis=1)\n",
    "final_df['label'] = data['label']\n",
    "final_df = final_df.fillna(0)\n",
    "\n",
    "# Save the final dataset\n",
    "final_df.to_csv('final_phishing_dataset.csv', index=False)\n",
    "print(\"Dataset saved as final_phishing_dataset.csv\")\n",
    "print(f\"Dataset shape: {final_df.shape}\")\n",
    "print(f\"Engineered features: {len(features_df.columns)}\")\n",
    "print(f\"BERT features: {len(bert_df.columns)}\")\n",
    "print(f\"Total features: {len(final_df.columns) - 1}\")  # -1 for label column\n",
    "\n",
    "# Save the BERT model info for later use\n",
    "import json\n",
    "bert_info = {\n",
    "    \"model_name\": \"bert-base-uncased\",\n",
    "    \"embedding_dimensions\": 768,\n",
    "    \"max_length\": 512\n",
    "}\n",
    "with open('bert_info.json', 'w') as f:\n",
    "    json.dump(bert_info, f)\n",
    "print(\"BERT model info saved as bert_info.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
