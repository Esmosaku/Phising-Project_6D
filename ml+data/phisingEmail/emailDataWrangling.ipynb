{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52849d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tldextract\n",
    "!pip install xgboost\n",
    "!pip install pandas numpy beautifulsoup4 textblob scikit-learn\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install scipy\n",
    "!pip install nltk\n",
    "!pip install wordcloud\n",
    "!pip install scipy\n",
    "!pip install nltk\n",
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7171b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import urllib.parse\n",
    "from collections import Counter\n",
    "import tldextract\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b4a1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating phising email dataset\n",
    "#Utilizing `7 Email Phising Datasets` and merging them into a single dataset (https://figshare.com/articles/dataset/Seven_Phishing_Email_Datasets/25432108)\n",
    "import glob\n",
    "\n",
    "# Path to your datasets; adjust the pattern if they are in a different folder or format\n",
    "dataset_files = glob.glob(\"7PhisingEmailsDataset/*.csv\")  # Example: all CSVs in a 'datasets' folder\n",
    "\n",
    "# List for storing the reduced DataFrames\n",
    "dataframes = []\n",
    "\n",
    "for file in dataset_files:\n",
    "    try:\n",
    "        # Read only the columns you need; ignore others (extra columns will be dropped)\n",
    "        df = pd.read_csv(file, usecols=['subject', 'body', 'label'])\n",
    "        dataframes.append(df)\n",
    "        print(f\"Loaded {file} with {len(df)} rows\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file}: {e}\")\n",
    "\n",
    "# Concatenate all reduced DataFrames into one\n",
    "merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "print(f\"\\nMerged dataset shape: {merged_df.shape}\")\n",
    "\n",
    "# Save as CSV\n",
    "merged_df.to_csv(\"merged_phishing_emails.csv\", index=False)\n",
    "print(\"Saved merged dataset to merged_phishing_emails.csv\")\n",
    "\n",
    "#Preview first few rows\n",
    "print(\"\\nSample data:\")\n",
    "print(merged_df.head())\n",
    "print(merged_df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff135810",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.8' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: '/Users/agamjotsingh/.pyenv/versions/3.11.8/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "#Next step is to take a look a the dataset and see if:\n",
    "    #1. The dataset is balanced\n",
    "    #2. The dataset is clean\n",
    "\n",
    "#Load the dataset\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv(\"merged_phishing_emails.csv\")\n",
    "print(f\"Dataframe Shape Before Making Changes: {df.shape}\")\n",
    "#Preview the dataset\n",
    "df.isnull().sum()\n",
    "df.dropna(inplace=True)\n",
    "df.info()\n",
    "df.head()\n",
    "\n",
    "#Check for duplicates\n",
    "df.duplicated().sum()\n",
    "df.drop_duplicates(inplace=True)\n",
    "print(f\"Dataframe Shape After Making Changes: {df.shape}\")\n",
    "\n",
    "#Check for balance \n",
    "df['label'].value_counts() \n",
    "\n",
    "df.to_csv(\"cleaned_phishing_emails.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636d5c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from textblob import TextBlob\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def count_keywords(text):\n",
    "    # counts occurrences of suspicious keywords in text\n",
    "    keywords = [\"urgent\", \"verify your account\", \"click here\", \"login now\", \"password reset\",\n",
    "                \"account suspended\", \"update your information\", \"confirm your identity\",\n",
    "                \"secure your account\", \"action required\"]\n",
    "    text = text.lower()\n",
    "    counts = {}\n",
    "    for word in keywords:\n",
    "        counts[\"count_\" + word.replace(\" \", \"_\")] = len(re.findall(r'\\b' + word + r'\\b', text))\n",
    "    return counts\n",
    "\n",
    "def check_greeting(text):\n",
    "    # checks for generic greetings in first 200 characters\n",
    "    greetings = [\"dear customer\", \"dear user\", \"hello sir\", \"hello madam\", \"dear client\"]\n",
    "    first_bit = text.lower()[:200]\n",
    "    for greeting in greetings:\n",
    "        if greeting in first_bit:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def get_sentiment(text):\n",
    "    # computes sentiment polarity and subjectivity\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity, blob.sentiment.subjectivity\n",
    "\n",
    "def persuasion_cues(text):\n",
    "    # counts gain and loss persuasion phrases\n",
    "    good_phrases = [\"win\", \"prize\", \"bonus\", \"reward\"]\n",
    "    bad_phrases = [\"lose\", \"suspended\", \"locked\", \"expired\"]\n",
    "    text = text.lower()\n",
    "    good_count = 0\n",
    "    bad_count = 0\n",
    "    for phrase in good_phrases:\n",
    "        good_count += len(re.findall(r'\\b' + phrase + r'\\b', text))\n",
    "    for phrase in bad_phrases:\n",
    "        bad_count += len(re.findall(r'\\b' + phrase + r'\\b', text))\n",
    "    return good_count, bad_count\n",
    "\n",
    "def get_lengths(subject, body):\n",
    "    return len(subject), len(body.split())\n",
    "\n",
    "def count_html_tags(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    return len(soup.find_all())\n",
    "\n",
    "def count_urls(text):\n",
    "    url_pattern = r'https?://[^\\s<>\"]+|www\\.[^\\s<>\"]+'\n",
    "    return len(re.findall(url_pattern, text))\n",
    "\n",
    "def count_attachments(text):\n",
    "    return text.lower().count(\"content-disposition: attachment\")\n",
    "\n",
    "def count_exclamation(text):\n",
    "    return text.count(\"!\")\n",
    "\n",
    "data = pd.read_csv('cleaned_phishing_emails.csv')\n",
    "all_features = []\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    subject = row['subject']\n",
    "    body = row['body']\n",
    "    features = {}\n",
    "\n",
    "    features.update(count_keywords(subject + ' ' + body))\n",
    "    features['generic_greeting'] = check_greeting(body)\n",
    "    polarity, subjectivity = get_sentiment(body)\n",
    "    features['polarity'] = polarity\n",
    "    features['subjectivity'] = subjectivity\n",
    "    good_count, bad_count = persuasion_cues(body)\n",
    "    features['good_phrases'] = good_count\n",
    "    features['bad_phrases'] = bad_count\n",
    "    sub_len, body_len = get_lengths(subject, body)\n",
    "    features['subject_length'] = sub_len\n",
    "    features['body_length'] = body_len\n",
    "    features['html_tags'] = count_html_tags(body)\n",
    "    features['url_count'] = count_urls(body)\n",
    "    features['attachment_count'] = count_attachments(body)\n",
    "    features['exclamation_count'] = count_exclamation(subject + ' ' + body)\n",
    "\n",
    "    all_features.append(features)\n",
    "\n",
    "features_df = pd.DataFrame(all_features)\n",
    "\n",
    "\n",
    "# Utilizying TFID (Term Frequence-Inverse Document Frequency). This is a technique that helps to reduce the dimensionality of the data by removing words that are common across all documents\n",
    "# In this case I have made it to use the top 2500 words and remove stop words (common words like \"the\", \"and\", \"is\", etc.)\n",
    "vectorizer = TfidfVectorizer(max_features=2500, stop_words='english')\n",
    "text_for_tfidf = data['subject'] + ' ' + data['body']\n",
    "tfidf_matrix = vectorizer.fit_transform(text_for_tfidf)\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "final_df = pd.concat([features_df, tfidf_df], axis=1)\n",
    "final_df['label'] = data['label']\n",
    "final_df = final_df.fillna(0)\n",
    "\n",
    "final_df.to_csv('final_phishing_dataset.csv', index=False)\n",
    "print(\"Dataset saved as final_phishing_dataset.csv\")\n",
    "\n",
    "from joblib import dump\n",
    "dump(vectorizer, 'tfidf_vectorizer.joblib')\n",
    "print(\"TF-IDF vectorizer saved as tfidf_vectorizer.joblib\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
